{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUl2c0EMnQ0T"
   },
   "source": [
    "## Summary:\n",
    "\n",
    "Whenever you make a purchase with a credit or debit card, it gets recorded in your account. The merchant information is an unstructured string with a 'store number' embedded somewhere in the string. In this project, I wrote scripts (non learning based vs learning based) to extract the correct store numbers from the merchant descriptors. I deleted some parts where the merchant descriptor is visible due to privacy concerns.\n",
    "\n",
    "**Method 1**: I made a non-learning model first because I thought it will be very effective for this particular assignment. I used regular expression library and several if and then statements to make a function that can pick out the store number from transaction information string.\n",
    "\n",
    "**Method 2**: I made a learning-based model using SpaCy library. I trained a transformer based neural network algorithm for NER in SpaCy library. I don't have a powerful GPU myself, so I couldn't design my own archetecture and experiment with pytorch.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hWhDchVkatel"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "import random\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "# from spacy.training.example import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QReeckLuYIfV",
    "outputId": "115bdaa4-8355-426e-80b3-8945b6e29725"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sFoBODngXQzA",
    "outputId": "5fc85339-5641-438b-a51d-61c544c0533c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VQvrDlPKcrjt"
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"Summer Internship - Homework Exercise.csv\")\n",
    "df = pd.read_csv(\"Summer Internship - Homework Exercise.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iYGhJeIHcr_t"
   },
   "outputs": [],
   "source": [
    "# setting up the dataframe for train, validation, and test set in pandas\n",
    "train = df.loc[df['dataset'] == 'train']\n",
    "validation = df.loc[df['dataset'] == 'validation']\n",
    "test = df.loc[df['dataset'] == 'test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4SJ0Oo0Hj-n"
   },
   "source": [
    "# **Non-Learning Based Model with Regular Expression**\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "i7Wzoe6_Va5s"
   },
   "outputs": [],
   "source": [
    "states = [ 'AK', 'AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA',\n",
    "           'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME',\n",
    "           'MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM',\n",
    "           'NV', 'NY', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX',\n",
    "           'UT', 'VA', 'VT', 'WA', 'WI', 'WV', 'WY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cTv-D-ZEVa5u"
   },
   "outputs": [],
   "source": [
    "# does string have number in it?\n",
    "def has_numbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "# function returns only digits from strong\n",
    "def digitonly(inputString):\n",
    "    return (''.join(filter(str.isdigit, inputString))).lstrip('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "39-FtgV1Va5u"
   },
   "outputs": [],
   "source": [
    "def store_number_extractor(string):\n",
    "    tokenized = word_tokenize(string)\n",
    "    store_number = ''\n",
    "    \n",
    "    # When store number is after '#'\n",
    "    if \"#\" in string:\n",
    "        for i in range(len(tokenized)):\n",
    "            if tokenized[i] == \"#\":\n",
    "                # filter only digits\n",
    "                # .lstrip to remove leading zero\n",
    "                return digitonly(tokenized[i+1])\n",
    "    \n",
    "    # when there are two numbers\n",
    "    for i in range(len(tokenized)):\n",
    "        if (i == len(tokenized)-1):\n",
    "            break\n",
    "        # if X##### (one letter) or CA#### (state) in tokenized\n",
    "        # then, return the token\n",
    "        if (has_numbers(tokenized[i])):\n",
    "            split = re.split('(\\d+)',tokenized[i])\n",
    "            if ((split[0] in states) or ((len(split[0]) == 1) and (not (split[0].isnumeric())))):\n",
    "                return split[0] + split[1]\n",
    "        if has_numbers(tokenized[i]) and has_numbers(tokenized[i+1]):\n",
    "            # if digit behind longer it is the code\n",
    "            front = digitonly(tokenized[i])\n",
    "            back = digitonly(tokenized[i+1])\n",
    "            if(len(front) < len(back)):\n",
    "                return back\n",
    "            else:\n",
    "                return front\n",
    "            \n",
    "        if tokenized[i].isnumeric() and tokenized[i+1].isnumeric():\n",
    "            return tokenized[i].lstrip('0')\n",
    "        \n",
    "        \n",
    "    # if X##### (one letter) or CA#### (state) in tokenized\n",
    "    #    then, return the token\n",
    "    for i in range(len(tokenized)):\n",
    "        if (has_numbers(tokenized[i])):\n",
    "            split = re.split('(\\d+)',tokenized[i])\n",
    "            if ((split[0] in states) or ((len(split[0]) == 1) and (not (split[0].isnumeric())))):\n",
    "                return split[0] + split[1]\n",
    "            \n",
    "        # if the store number has state code in the front\n",
    "        #if (re.split('(\\d+)',tokenized[i])[0] in states)\n",
    "        \n",
    "    \n",
    "        \n",
    "    # the rest, when store number is the only number    \n",
    "    for i in range(len(tokenized)):   \n",
    "        if tokenized[i].isnumeric():\n",
    "            return tokenized[i].lstrip('0')\n",
    "    return (''.join(filter(str.isdigit, TreebankWordDetokenizer().detokenize(tokenized)))).lstrip('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JXV5YopUVa5t",
    "outputId": "7e8e0587-081d-48b1-8f50-dc28c6a84aee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BP', '#', '5998869CK', 'ST']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the function\n",
    "string = \"BP#5998869CK ST\"\n",
    "word_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "fXpkjCWcVa5v",
    "outputId": "e8ad5804-105b-48e6-fde0-04513cd4ff24"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'5998869'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_number_extractor(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OnV9dQZ_Va5x",
    "outputId": "b110dc2c-78ca-4d4a-b08d-0ef9bf2835fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy score for the training set\n",
    "sum((train['transaction_descriptor'].apply(store_number_extractor) == train['store_number']))/len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DvhqgRT5Va5y",
    "outputId": "5e486dc0-13cb-4824-e185-6bb68d3040f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing it on validation set:\n",
    "sum((validation['transaction_descriptor'].apply(store_number_extractor) == validation['store_number']))/len(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aF-7iLHPrivW",
    "outputId": "0106239a-72c3-43aa-9ed6-3bbd9656d7b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and on test set:\n",
    "sum((test['transaction_descriptor'].apply(store_number_extractor) == test['store_number']))/len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFAG9q9QVa5z"
   },
   "source": [
    "Respectable performance for the non-learning based 'filter' I've created by just figuring out the patterns myself.\n",
    "\n",
    "\n",
    "Now, \n",
    "# **Learning-Based Entity Extraction Model**  (using SpaCy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ua5LpfRIVkcI"
   },
   "outputs": [],
   "source": [
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pcpG2VskVa5z",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA = []\n",
    "# setting up the data (json structure)\n",
    "for index, row in train.iterrows():\n",
    "    substringstart = row[\"transaction_descriptor\"].find(row[\"store_number\"])\n",
    "    substringend = substringstart + len(row[\"store_number\"])\n",
    "    TRAIN_DATA.append((row[\"transaction_descriptor\"], {'entities': [(substringstart, substringend, row[\"store_number\"])]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uEQSG31tVa50",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# new formatting for Spacy v3\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "nlp = spacy.blank(\"en\") # load a new spacy model\n",
    "db = DocBin() # create a DocBin object\n",
    "\n",
    "for text, annot in tqdm(TRAIN_DATA): # data in previous format\n",
    "    doc = nlp.make_doc(text) # create doc object from text\n",
    "    ents = []\n",
    "    for start, end, label in annot[\"entities\"]: # add character indexes\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    doc.ents = ents # label the text with the ents\n",
    "    db.add(doc)\n",
    "\n",
    "db.to_disk(\"./train.spacy\") # save the docbin object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZoBpOtblVoIp"
   },
   "outputs": [],
   "source": [
    "# download transfer model from spacy\n",
    "!python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jQUc-tTYV6A3",
    "outputId": "24e8c4ed-d172-46bd-f462-7fbce8a09c48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.cuda.is_available>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g5T5JKa5WMnl",
    "outputId": "d9bb91bf-c8d9-42c7-8836-544d13ce0d3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHu7BjepWX0r"
   },
   "outputs": [],
   "source": [
    "# install hungging face transfermer library\n",
    "!pip install -U spacy[cuda92,transformers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fp0NhrntWlsA"
   },
   "outputs": [],
   "source": [
    "# !pip install cupy\n",
    "!curl https://colab.chainer.org/install | sh -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PlTWPKYNWpr5",
    "outputId": "97906852-a25c-4fb0-8df9-c56c88c8d7fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 0: export: `=': not a valid identifier\n",
      "/bin/bash: line 0: export: `/usr/local/cuda-9.2': not a valid identifier\n"
     ]
    }
   ],
   "source": [
    "!export CUDA_PATH = \"/usr/local/cuda-9.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "B_ILG8n2Wy31"
   },
   "outputs": [],
   "source": [
    "!export LD_LIBRARY_PATH=$CUDA_PATH/lib64:$LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jnnd8FSwaD48",
    "outputId": "75d6562c-1c37-4c99-edf9-ab2b284179b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "# initalize the config file for our data\n",
    "!python -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "smAbJ39Jc4V3",
    "outputId": "cff6cf9d-84e5-441e-9a22-bafee9d1097c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2022-05-08 20:58:20,750] [INFO] Set up nlp object from config\n",
      "[2022-05-08 20:58:20,759] [INFO] Pipeline: ['transformer', 'ner']\n",
      "[2022-05-08 20:58:20,763] [INFO] Created vocabulary\n",
      "[2022-05-08 20:58:20,764] [INFO] Finished initializing nlp object\n",
      "Downloading: 100% 481/481 [00:00<00:00, 544kB/s]\n",
      "Downloading: 100% 878k/878k [00:00<00:00, 30.0MB/s]\n",
      "Downloading: 100% 446k/446k [00:00<00:00, 17.7MB/s]\n",
      "Downloading: 100% 1.29M/1.29M [00:00<00:00, 27.4MB/s]\n",
      "Downloading: 100% 478M/478M [00:06<00:00, 73.7MB/s]\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[2022-05-08 20:58:38,614] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  -------------  --------  ------  ------  ------  ------\n",
      "  0       0         152.41    400.95    0.00    0.00    0.00    0.00\n",
      "200     200        7092.96  22625.02    0.00    0.00    0.00    0.00\n",
      "400     400           0.68   3408.81    0.00    0.00    0.00    0.00\n",
      "600     600           0.00   3405.45    0.00    0.00    0.00    0.00\n",
      "800     800           0.00   3402.95    0.00    0.00    0.00    0.00\n",
      "1000    1000           0.00   3400.19    0.00    0.00    0.00    0.00\n",
      "1200    1200           0.00   3397.10    0.00    0.00    0.00    0.00\n",
      "1400    1400           0.00   3393.58    0.00    0.00    0.00    0.00\n",
      "1600    1600           0.00   3389.49    0.00    0.00    0.00    0.00\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "output/model-last\n"
     ]
    }
   ],
   "source": [
    "# training spacy NER model based on transformer archetecture\n",
    "!python -m spacy train --gpu-id 0 config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./validation.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "S7Q4J0beVa51"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"output/model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y95mITGaVa51",
    "outputId": "ca3719cb-765d-4547-8ce7-37c43edfe4a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy of the trained model on the test set\n",
    "accuracy = []\n",
    "for index, row in test.iterrows():\n",
    "    doc = nlp(row['transaction_descriptor'])\n",
    "    for ent in doc.ents:\n",
    "        predicted = ent.text\n",
    "    accuracy.append((predicted == row[\"store_number\"]))\n",
    "accuracy = sum(accuracy)/len(accuracy)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Affinity_Solutions_Summer_Internship_Homework_Exercise.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
